{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#from model.model import model_load\n",
    "#from model.model import model_predict\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "#from model.cslib import fetch_ts, engineer_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ts(data_dir, clean=False):\n",
    "    \"\"\"\n",
    "    convenience function to read in new data\n",
    "    uses csv to load quickly\n",
    "    use clean=True when you want to re-create the files\n",
    "    \"\"\"\n",
    "\n",
    "    ts_data_dir = os.path.join(data_dir,\"ts-data\")\n",
    "    \n",
    "    if clean:\n",
    "        shutil.rmtree(ts_data_dir)\n",
    "    if not os.path.exists(ts_data_dir):\n",
    "        os.mkdir(ts_data_dir)\n",
    "\n",
    "    ## if files have already been processed load them        \n",
    "    if len(os.listdir(ts_data_dir)) > 0:\n",
    "        print(\"... loading ts data from files\")\n",
    "        return({re.sub(\"\\.csv\",\"\",cf)[3:]:pd.read_csv(os.path.join(ts_data_dir,cf)) for cf in os.listdir(ts_data_dir)})\n",
    "\n",
    "    ## get original data\n",
    "    print(\"... processing data for loading\")\n",
    "    df = fetch_data(data_dir)\n",
    "\n",
    "    ## find the top ten countries (wrt revenue)\n",
    "    table = pd.pivot_table(df,index='country',values=\"price\",aggfunc='sum')\n",
    "    table.columns = ['total_revenue']\n",
    "    table.sort_values(by='total_revenue',inplace=True,ascending=False)\n",
    "    top_ten_countries =  np.array(list(table.index))[:10]\n",
    "\n",
    "    file_list = [os.path.join(data_dir,f) for f in os.listdir(data_dir) if re.search(\"\\.json\",f)]\n",
    "    countries = [os.path.join(data_dir,\"ts-\"+re.sub(\"\\s+\",\"_\",c.lower()) + \".csv\") for c in top_ten_countries]\n",
    "\n",
    "    ## load the data\n",
    "    dfs = {}\n",
    "    dfs['all'] = convert_to_ts(df)\n",
    "    for country in top_ten_countries:\n",
    "        country_id = re.sub(\"\\s+\",\"_\",country.lower())\n",
    "        file_name = os.path.join(data_dir,\"ts-\"+ country_id + \".csv\")\n",
    "        dfs[country_id] = convert_to_ts(df,country=country)\n",
    "\n",
    "    ## save the data as csvs    \n",
    "    for key, item in dfs.items():\n",
    "        item.to_csv(os.path.join(ts_data_dir,\"ts-\"+key+\".csv\"),index=False)\n",
    "        \n",
    "    return(dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading ts data from files\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(\"cs-train\")\n",
    "\n",
    "ts_all = fetch_ts(data_dir,clean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df,training=True):\n",
    "    \"\"\"\n",
    "    for any given day the target becomes the sum of the next days revenue\n",
    "    for that day we engineer several features that help predict the summed revenue\n",
    "    \n",
    "    the 'training' flag will trim data that should not be used for training\n",
    "    when set to false all data will be returned\n",
    "    \"\"\"\n",
    "\n",
    "    ## extract dates\n",
    "    dates = df['date'].values.copy()\n",
    "    dates = dates.astype('datetime64[D]')\n",
    "\n",
    "    ## engineer some features\n",
    "    eng_features = defaultdict(list)\n",
    "    previous =[7, 14, 28, 70]  #[7, 14, 21, 28, 35, 42, 49, 56, 63, 70]\n",
    "    y = np.zeros(dates.size)\n",
    "    for d,day in enumerate(dates):\n",
    "\n",
    "        ## use windows in time back from a specific date\n",
    "        for num in previous:\n",
    "            current = np.datetime64(day, 'D') \n",
    "            prev = current - np.timedelta64(num, 'D')\n",
    "            mask = np.in1d(dates, np.arange(prev,current,dtype='datetime64[D]'))\n",
    "            eng_features[\"previous_{}\".format(num)].append(df[mask]['revenue'].sum())\n",
    "\n",
    "        ## get get the target revenue    \n",
    "        plus_30 = current + np.timedelta64(30,'D')\n",
    "        mask = np.in1d(dates, np.arange(current,plus_30,dtype='datetime64[D]'))\n",
    "        y[d] = df[mask]['revenue'].sum()\n",
    "\n",
    "        ## attempt to capture monthly trend with previous years data (if present)\n",
    "        start_date = current - np.timedelta64(365,'D')\n",
    "        stop_date = plus_30 - np.timedelta64(365,'D')\n",
    "        mask = np.in1d(dates, np.arange(start_date,stop_date,dtype='datetime64[D]'))\n",
    "        eng_features['previous_year'].append(df[mask]['revenue'].sum())\n",
    "\n",
    "        ## add some non-revenue features\n",
    "        minus_30 = current - np.timedelta64(30,'D')\n",
    "        mask = np.in1d(dates, np.arange(minus_30,current,dtype='datetime64[D]'))\n",
    "        eng_features['recent_invoices'].append(df[mask]['unique_invoices'].mean())\n",
    "        eng_features['recent_views'].append(df[mask]['total_views'].mean())\n",
    "\n",
    "    X = pd.DataFrame(eng_features)\n",
    "    ## combine features in to df and remove rows with all zeros\n",
    "    X.fillna(0,inplace=True)\n",
    "    mask = X.sum(axis=1)>0\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    dates = dates[mask]\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if training == True:\n",
    "        ## remove the last 30 days (because the target is not reliable)\n",
    "        mask = np.arange(X.shape[0]) < np.arange(X.shape[0])[-30]\n",
    "        X = X[mask]\n",
    "        y = y[mask]\n",
    "        dates = dates[mask]\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return(X,y,dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,dates = engineer_features(ts_all['all'])\n",
    "        \n",
    "## Perform a train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regressor Method - Performance Indicators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time =  00:00:16\n",
      "mae = 10921\n",
      "mse = 235484964\n",
      "r2_score = 0.964\n",
      "best params = {'rf__criterion': 'mse', 'rf__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "param_grid_rf = {\n",
    "    'rf__criterion': ['mse','mae'],\n",
    "    'rf__n_estimators': [10,15,20,25,50,100]\n",
    "    }\n",
    "\n",
    "time_start = time.time()\n",
    "pipe_rf = Pipeline(steps=[('scaler', StandardScaler()), ('rf', RandomForestRegressor())])\n",
    "\n",
    "#grid = GridSearchCV(pipe_rf, param_grid=param_grid_rf, cv=5, iid=False, n_jobs=-1)\n",
    "grid = GridSearchCV(pipe_rf, param_grid=param_grid_rf, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "rf_mae =  mean_absolute_error(y_test, y_pred)\n",
    "rf_mse =  mean_squared_error(y_test, y_pred)\n",
    "rf_r2_score = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"train time = \", time.strftime('%H:%M:%S', time.gmtime(time.time()-time_start)))\n",
    "print(\"mae = {:.0f}\".format(rf_mae))\n",
    "print(\"mse = {:.0f}\".format(rf_mse))\n",
    "print(\"r2_score = {:.3f}\".format(rf_r2_score))\n",
    "print(\"best params =\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regressor Method - Performance Indicators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time =  00:00:07\n",
      "mae = 16324\n",
      "mse = 459512612\n",
      "r2_score = 0.930\n",
      "best params = {'gb__criterion': 'mse', 'gb__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "param_grid_gb = {\n",
    "    'gb__criterion': ['mse','mae'],\n",
    "    'gb__n_estimators': [10,15,20,25,50,100]\n",
    "    }\n",
    "\n",
    "time_start = time.time()\n",
    "pipe_gb = Pipeline(steps=[('scaler', StandardScaler()), ('gb', GradientBoostingRegressor())])\n",
    "\n",
    "#grid = GridSearchCV(pipe_gb, param_grid=param_grid_gb, cv=5, iid=False, n_jobs=-1)\n",
    "grid = GridSearchCV(pipe_gb, param_grid=param_grid_gb, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "gb_mae =  mean_absolute_error(y_test, y_pred)\n",
    "gb_mse =  mean_squared_error(y_test, y_pred)\n",
    "gb_r2_score = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"train time = \", time.strftime('%H:%M:%S', time.gmtime(time.time()-time_start)))\n",
    "print(\"mae = {:.0f}\".format(gb_mae))\n",
    "print(\"mse = {:.0f}\".format(gb_mse))\n",
    "print(\"r2_score = {:.3f}\".format(gb_r2_score))\n",
    "print(\"best params =\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Regressor Method - Performance Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time =  00:00:02\n",
      "mae = 12514\n",
      "mse = 445528579\n",
      "r2_score = 0.932\n",
      "best params = {'dt__criterion': 'mse', 'dt__max_depth': 10, 'dt__min_samples_leaf': 2}\n"
     ]
    }
   ],
   "source": [
    "param_grid_dt = {\n",
    "    'dt__criterion': ['mse','mae'],\n",
    "    'dt__max_depth': [5,10,20,50],\n",
    "    'dt__min_samples_leaf': [1,2,3,4,5]\n",
    "    }\n",
    "\n",
    "time_start = time.time()\n",
    "pipe_ts = Pipeline(steps=[('scaler', StandardScaler()), ('dt', DecisionTreeRegressor())])\n",
    "\n",
    "#grid = GridSearchCV(pipe_ts, param_grid=param_grid_dt, cv=5, iid=False, n_jobs=-1)\n",
    "grid = GridSearchCV(pipe_ts, param_grid=param_grid_dt, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "dt_mae =  mean_absolute_error(y_test, y_pred)\n",
    "dt_mse =  mean_squared_error(y_test, y_pred)\n",
    "dt_r2_score = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"train time = \", time.strftime('%H:%M:%S', time.gmtime(time.time()-time_start)))\n",
    "print(\"mae = {:.0f}\".format(dt_mae))\n",
    "print(\"mse = {:.0f}\".format(dt_mse))\n",
    "print(\"r2_score = {:.3f}\".format(dt_r2_score))\n",
    "print(\"best params =\", grid.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
